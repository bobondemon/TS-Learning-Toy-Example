{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier Experiments with MNIST\n",
    "A typical CNN classifier for MNIST. Could makes 99% in both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "# from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Shuffle arrays or sparse matrices in a consistent way\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "dataPath='../dataset/MNIST_data/'\n",
    "mnist = input_data.read_data_sets(dataPath, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]\n",
    "img_width = 28\n",
    "img_height = 28\n",
    "images = mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having 5417 and 5454 for 6 and 9 respectively\n"
     ]
    }
   ],
   "source": [
    "# read the labels, 6 --> class 0; 9 --> class 1\n",
    "y1hot_all = mnist.train.labels\n",
    "y_all = np.argmax(y1hot_all,axis=1)\n",
    "idx6 = np.nonzero(y_all==6)[0]\n",
    "idx9 = np.nonzero(y_all==9)[0]\n",
    "idx69 = np.concatenate([idx6, idx9])\n",
    "print('Having {} and {} for 6 and 9 respectively'.format(len(idx6),len(idx9)))\n",
    "y = np.copy(y_all)\n",
    "y[idx6] = 0\n",
    "y[idx9] = 1\n",
    "y = y[idx69]\n",
    "y = y[...,np.newaxis]\n",
    "y1hot = [[int(x[6]), int(x[9])] for x in y1hot_all[idx69]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allidx = set(range(len(y_all)))\n",
    "restidx = list(set.difference(allidx,set(idx69)))\n",
    "#print(len(restidx),len(idx69),len(allidx))\n",
    "#assert(len(restidx)+len(idx69)==len(allidx))\n",
    "y1hot_rest_flip = [[1, 0] for x in y1hot_all[restidx]]\n",
    "#print(len(y1hot_rest_flip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Input to CNN) Images with shape (10871, 28, 28, 1)\n",
      "(Input to CNN) Rest of Images with shape (44129, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]\n",
    "# extract 6, 9 as training images. The rest of images are for unsupervised training\n",
    "n_classes = 2\n",
    "x = images[idx69]\n",
    "x_num, _ = x.shape\n",
    "assert(len(x)==len(y))\n",
    "x = np.reshape(x,(x_num,img_height,img_width))\n",
    "x = x[...,np.newaxis]\n",
    "print('(Input to CNN) Images with shape {}'.format(x.shape))\n",
    "restidx = list( set.difference( set(range(len(images))), set(idx69) ) )\n",
    "assert(len(restidx)+len(x)==len(images))\n",
    "x_rest = images[restidx]\n",
    "x_rest_num, _ = x_rest.shape\n",
    "x_rest = np.reshape(x_rest,(x_rest_num,img_height,img_width))\n",
    "x_rest = x_rest[...,np.newaxis]\n",
    "print('(Input to CNN) Rest of Images with shape {}'.format(x_rest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAEpCAYAAADieNHgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xnc1lP+x/H3ISUtlG2iCJEpWyZ7g6FGzNj3JctIGPuW\nfRsyGMuMkSWMYpAl1I/BpLFmJmRJhcqQRBGZJFt8f39093XOyX3d1/K9rutc13k9H4/74Xzuc1/X\n99z35z5X99d1PueYJEkEAAAAAAjHUtUeAAAAAADAxY0aAAAAAASGGzUAAAAACAw3agAAAAAQGG7U\nAAAAACAw3KgBAAAAQGC4UQMAAACAwHCjViBjTHtjzEPGmC+NMdONMQdVe0zIHnmOB7mOA3mOB7mO\nA3mOgzHmeGPMy8aYb4wxQ6s9nkprVu0B1KDBkr6VtKqkTSQ9aox5PUmSSdUdFjJGnuNBruNAnuNB\nruNAnuPwoaRLJe0kqWWVx1JxJkmSao+hZhhjWkmaK2mDJEmmNHzuDkkfJklyVlUHh8yQ53iQ6ziQ\n53iQ6ziQ5/gYYy6V1DFJksOrPZZKYuljYdaTtHDxi0KD1yV1r9J4UB7kOR7kOg7kOR7kOg7kGVHg\nRq0wrSXN8z43T1KbKowF5UOe40Gu40Ce40Gu40CeEQVu1AozX1Jb73PLS/qiCmNB+ZDneJDrOJDn\neJDrOJBnRIEbtcJMkdTMGLOu9bmNJVG4Wl/IczzIdRzIczzIdRzIM6LAZiIFMsYMl5RI6i+ph6RH\nJW3NLkP1hTzHg1zHgTzHg1zHgTzHwRjTTIt2qb9QUkdJR2lRfeLCqg6sQnhHrXC/16LtQT+WdLek\nY3lRqEvkOR7kOg7kOR7kOg7kOQ7nSfpK0lmSDmlon1fVEVUQ76gBAAAAQGB4Rw0AAAAAAsONGgAA\nAAAEhhs1AAAAAAhMSTdqxpi+xpi3jTHTjDFnZTUohIdcx4E8x4Ncx4E8x4Ncx4E8RyZJkqI+JC0t\n6R1Ja0tqLul1Sd2aeEzCR5gfWea62t8LHzk/PmFOx/HBnI7jg3+n4/lgTsfxwZyO5yOf+61S3lHb\nXNK0JEn+myTJt5KGS9q9hOdDuMh1/Zieo488x4Ncx4E8x4Ncx4E8R6aUG7XVJc2w4g8aPucwxgww\nxrxsjHm5hGuhuprMNXmuC8zpeDCn48CcjgdzOg7M6cg0K/cFkiQZImmIJBljknJfD9VBnuNBruNA\nnuNBruNAnuNBrutHKe+ozZTUyYo7NnwO9Ydcx4E8x4Ncx4E8x4Ncx4E8R6aUG7WXJK1rjFnLGNNc\n0gGSRmUzLASGXMeBPMeDXMeBPMeDXMeBPEem6KWPSZIsNMYcL+kJLdqF5m9JkkzKbGQIBrmOA3mO\nB7mOA3mOB7mOA3mOj2nYurMyF2OdbLCSJDFZPRd5Dtr4JEl6ZvVk5DpczOk4ZJlniVyHjDkdB+Z0\n8U4//XQnPv/889N29+7dnb4PPvigImPKJZ9cl3TgNQAAAAAge9yoAQAAAEBguFEDAAAAgMCU/Rw1\nAAAAAKiktm3bpu3WrVtXcSTF4x01AAAAAAgMN2oAAAAAEBhu1AAAAAAgMNSoAQAAAKgrlTwrulx4\nRw0AAAAAAsONGgAAAAAEhqWPAAAAAOqKMSZtr7feek7fW2+9VenhFIV31AAAAAAgMNyoAQAAAEBg\nuFEDAAAAgMDURY3a0ksv7cQHHHBA2r7jjjuKft7hw4c78b/+9a9Gn/f777934h9++KHo6yI/22+/\nfdq+8MILG+0rxdNPP+3EF198cc5+lMcKK6yQtk877TSn709/+pMT77XXXmn7tttuc/ruv/9+J953\n333T9ogRI5y+Cy64wIlrZT17yFZccUUnfvzxx5140003bfSx7777rhO//PLLafvcc891+j7//HMn\n/vTTTwsaJ7J35plnOrE9T3v27Fn08x5yyCFO/O9//zttv/fee0U/L4rTpUsXJx4/fnzabtWqVc7H\n2vVE06ZNc/pGjhzpxEOHDm30eSZPntzUMBEJe3v+DTfc0OkbNWpUpYdTFN5RAwAAAIDAcKMGAAAA\nAIHhRg0AAAAAAmPs9Ztlv5gxmVysTZs2TjxmzBgn/sUvfpG2/e9vwYIFTtyiRYu03axZ8SV7f//7\n3534ww8/dOKbb745bc+aNcvp+/rrr4u+blaSJDFNf1V+ssqz76KLLnJivy4tNE3Vs1Wpvm18kiTF\nF4R4ypVr3z777JO27733Xqdv4MCBTnz88cen7TXWWKPoa/qvFa+99poT2zUSfq3VzJkzi75uVkKc\n07/+9a+d+LHHHitkDE5sv7bPnTs352P79++ftseOHev0ffLJJ3mPIURZ5lkq35yeMGGCE3fv3r0c\nl3H49eudOnVy4hkzZpR9DFkKcU77Onbs6MRvvPFG2p44caLT5/+dZO8LYO81IEndunVz4p///OeN\njuHSSy91Yv+6//d//5e2v/nmm0afp1pqZU6H6PTTT3fiK6+8Mm2ff/75Tt+gQYMqMqZc8sk176gB\nAAAAQGC4UQMAAACAwNTk0sfLL7/cic8444xGv9ZfknjYYYc58T333JO299tvvwxG1zR/61h/e3F7\nzJXa5r8WllTk+l31lxkWwl9CaS9JfOaZZ3I+drvttnPiQo4FsMfsL+sso5pc+ti3b9+0bS9bkZb8\nvVhqqR///9Pdd9/t9L3++ut5X/Ooo45yYn/Z1LLLLpu2/eUzBx98sBM/9NBDeV83KyHO6aaWPtpz\nwt+O27f33nunbXtprCSttdZaTmwvcfeXpfpLkP3fmUKWZ1ZDrSyT8pc+rrPOOmnb/5n7DjrooLRt\nz7umTJo0yYm//PJLJ7711ludeNiwYWl74cKFeV+nUkKc0762bds68X//+9+0ff311zt9hfy752/t\nb8f+Mlr/3+XddtvNiTt06JC2X3rpJafP/l2TpPnz5+c9xqzUypwO0RFHHOHE9hE9LH0EAAAAAGSC\nGzUAAAAACAw3agAAAAAQmJqsUfvVr37lxE8++WSjXzt9+nQnXnvttZ24GjVqTTnxxBPT9o033uj0\nlatmLcS1701tx19sjddTTz3lxH5dWa4ataauYz+X/7y56tn8OpmmtvYvQU3WqNn+85//OPFmm23W\n6NeOHj3aif05Pm/evLyv27On+2M799xz07ZfA+Fv5W8fGVIpIc5pv0bNP9bg1VdfTdul/Mz8x9q/\nIz169HD6/Po2/1gGu6Z4/PjxTp9fX9zUMQHlUCv1LOPGjXPiRx99NG3/4Q9/yPnYPn36pO2HH37Y\n6ctVs5brSIefMmfOnLRt10BKS47/u+++y/lc5RDinPb5/+7ZNZ5nnnmm03fdddeVYwhN2n///dP2\nNtts4/S1a9fOie06Rr/G+fPPPy/D6GpnTofI/xt/2rRpadv/fTv55JMrMqZcqFEDAAAAgBrU5I2a\nMeZvxpiPjTETrc+1N8aMNsZMbfhvu1zPgdpAruNAnuNBruNAnuNBruNAnrFYPu+oDZXU1/vcWZLG\nJEmyrqQxDTFq31CR6xgMFXmOxVCR6xgMFXmOxVCR6xgMFXmG8qxRM8Z0lvRIkiQbNMRvS9o+SZKP\njDEdJD2dJEnXPJ4nk3WyyyyzjBPba90laccdd0zb/hkYfo3Kt99+m7b9Mxb8mqJS2HUO9hke0pJr\nom12vZpUvpq1xetks8h16DVqpdRl+jUPpbDX8vt1c74Mz1wbnyRJz9DmdCF+85vfOLF99pGUez75\nNUR2jepll13m9M2aNcuJ/d+bFVZYIW379SurrbaaE2+xxRZp2z9HsVxCnNO+PfbYw4lHjBiRtv3z\ncO64445yDGEJAwYMcGK7Vmnbbbd1+vxzuQ488MC07ddHlkuWeW54XFlybZ+bJkl33XVX2r7iiiuc\nvlznDu68885OfO211zrxuuuum7YLrVHL5fTTT3di+++OKVOmFP28haiFOe2bPXt22vZff7feemsn\n/uyzz/J+3tVXXz1t26/jkjRx4kQnfuONN5x46NChafurr75y+s477zwnvuSSS9K2P6d33313J/af\nq1i1MqdD1KZNGye2fxfsGlSpOrXjvnLWqK2aJMlHDe1ZklYt8nkQPnIdB/IcD3IdB/IcD3IdB/Ic\noWalPkGSJEmuu3VjzABJAxrrR+3IlWvyXD+Y0/FgTseBOR0P5nQcmNPxqMmlj75Ctut/7733nHiT\nTTZJ22ussYbT949//MOJO3bsWOQIs7P88ss7sb+0s1ghLqko19LHpp43l3Jto+9vaZxrKWSJyy9r\nfumjz9823z5SIdfW3U258847nXj48OGNfq2/vbi/pMJecm0v7yunEOe0b5VVVnHic845J20feeSR\nTp+/THLMmDFFXfOCCy5w4latWjnxv/71LyeeOnVq2vZfb/3t+Q855JC0/eCDDzp9p556qhPPmDEj\nzxHnVqvLpOyjDr755hunz18Sl4s/10477bS0bS9FlUorFfCPkrCXvVbqaJ9amNM+u3zjmmuucfr8\nUo4TTjihqGs8++yzTuxvuW9v0S5Jhx12WNr2j3vxtW/fPm2/8MILTp8/h+1jJEpRq3Pa5s8Jf5mo\nzS8zuP/++524qRzl8sorrzTat+mmmxb9vFkp59LHUZIW/6YfJmlkkc+D8JHrOJDneJDrOJDneJDr\nOJDnCOWzPf89kv4tqasx5gNjzJGSLpfUxxgzVVLvhhg1jlzHgTzHg1zHgTzHg1zHgTxjsSZr1JIk\nObCRrh0b+TxqFLmOA3mOB7mOA3mOB7mOA3nGYnnVqGV2sTKtk23RooUTjxo1Km337t0752NHjvzx\nnePDDz/c6WvevLkT28/Vr18/p8/ftnm55ZbLed1ilbtGLQuVqlGz+XWKxdaKSW59mF87Vgi/fi2r\nOroSv9fxSZL0bPrL8hNCjZrP3k7ZrnmSlpwvds1K69atnb6llip2ZfiSqlmjloVK5dneev2RRx5x\n+vzcdevWLW3PnDkz72v4tW1NzXH7uldeeaXT5//O2EevHHrooU6fv2W4fcxEKfVqWeZZqlyu7de4\nM844w+mzt+6XpKOPPjptF/I3y1//+lcn9v+Nb9mypRMXUv9rj+O6665z+vx6xKzU4py2/47y6/y6\ndOnixP4+Afmyt+qXlqzxXnvttZ3YrjXz9y2w/370bbzxxk581FFHObFdG/fOO+80PuAm1Mqc9ueL\n/XeKXwtcig8++CBt+3XBN910kxN36tTJie36RP8IrkGDBmU1xKKVs0YNAAAAAFAm3KgBAAAAQGC4\nUQMAAACAwNRFjZrPPhvNrynYccfG6zDvvvtuJ/br0HLx6xz8Nfc/+9nPGn2s32efKXHVVVc5ff75\nEgsXLsx7jLnUwtr3Qn5XSzlrzM6ln9dCzlwrhF/Plus6WZyjVsoTeGMJrkbNPiPnb3/7m9PXvXt3\nJ37rrbfS9sknn+z0+ed2+fUJbdu2bXQMU6ZMceJf//rXaTur87OaUgtz2mfXGx9xxBFO3w033ODE\n48aNS9u77LKL0zd37txGr7Heeus5sf/Yvffe24lXW221tL3WWms1+ryFevXVV9O2fxZYIWqlniUX\nv5bs97//vROfcsopads/3zBXrpvin3W32267pe1CXmf9+kP/tSIrtTinbf5rrP/v3hNPPJG2Bw4c\n6PT5tWS5rLnmmk7s1ybaZ2/6Z/b5+wDYvwf+3yALFixw4i233DJtT548Oe/x+mplTvu1f3Y92Kef\nfur0+XPa/9nZ/DPX7Bpv/9/d6dOnO/G9997rxPbv0WOPPeb02XXC1UKNGgAAAADUIG7UAAAAACAw\ndbn00darVy8n9rfGXmmlldL2vHnznL5bb73Vif3ljFnxt6i13z6ulFpYUuEvQ/S34LX5W9b7W9pn\nxd9Gf7vttkvbpWztnwtLH3NbYYUV0ra//OKAAw5wYn8pcS6dO3d24uOOOy5t+0d5XHvttU5cyLKd\nrNTCnC7Evvvu68R33HFH2vaXgO+1115OPHr06KKvay99PP30052+3/3ud07cpk2btO3PU//f2sGD\nB6ftE088sejx1coyqVy6du3qxPb26ZI7p6+//nqnzz6OQ5LmzJlT9Djs4yGOOeaYnF9rL2e2l+xJ\nSy6bzkq9zenzzjvPie2lkPZyV0m68cYbnfi7777LZAyrrLKKE/tL9Ox/0/05/NJLLzmxvYTX3lK+\nULUyp3MtfbSXdkulLe9eeuml07a/jPXss8/OOSZblmPKCksfAQAAAKAGcaMGAAAAAIHhRg0AAAAA\nAlP3NWq+bbfd1ontGhW7Xk2S/ve//zmxvV30yJEjyzC66qnFte92jVpT9WD22ne/rqxcctWvSYXV\nsNk1dn79XYHqvkbNrhN68cUXnb5XXnnFiQ8++OCKjKkaanFOF8Lepv2ggw5y+vytn+2aoTFjxmQ2\nho4dOzqxXwNp22GHHZzYPkbik08+KXoMtVLPUojVV1/diV9//fW03b59e6dv7NixTvzb3/42bfv/\nhmfJ3gL+s88+c/q++OKLslyz3uf0Dz/8kLb9v039Y4xKmTOhq5U5veKKKzqxXbO38sorO332FvvS\nklvlF6tDhw5O/NBDDznx5ptvnrapUQMAAAAAZIIbNQAAAAAIDDdqAAAAABCY6GrUfPY5Hscff7zT\n56+xtde733bbbU5fuc5Yq5RaX/vun6mWq/6rUmesBarua9Rsfl2gX59kn7c1atSoioypUmp9Tjel\nZcuWafucc85x+vx4/vz5aXufffZx+ko5Yy0EtVLPUgr7zMLLLrvM6WvdurUTv/baa2nbrwssZ81a\nJdT7nO7Tp0/a9v/GmjlzphMPHz48bf/lL38p78AqrFbntP3aet999zl9M2bMcOItt9wybX/00UeZ\njcE/6/LKK69M29SoAQAAAAAywY0aAAAAAASGGzUAAAAACEz0NWq23XbbzYn98xhsuc5Yk2rvnLV6\nW/teyO+1X6NW4jlloYuqRs0/j8k+50Vyz3I58sgjKzKmSqm3OZ1LixYtnNg+N1GSBg4cmLb9M67s\nOkUp23PWKqFW61mKteOOOzrx1Vdf7cQbbrhh2n7hhRecvj333NOJ58yZk/Hoyqve53SrVq3S9qWX\nXur0HX300U5sn5V49tlnO3233HJLGUZXObU6p+3X4VyvwZL0xhtvpO2ddtrJ6Zs1a1bRY+jfv78T\nDxkyJG2/++67Tt9GG23kxF9++WXR1y0WNWoAAAAAUIO4UQMAAACAwLD00bLccss5ce/evZ3YfgvV\n37p/3rx5Tnz44Yc7cehLIettSYW/Pb+/fX8u9lv2F110UUYjCkZUSx99kyZNcuLmzZun7S222MLp\n++yzzyoypnKptzldijPPPDNt+3N66tSpTnzaaac5cejb99fqMqmsbLLJJk78zDPPpO02bdo4ff6W\n4QcccED5BlYGMc/pe++914l33XXXtD1lyhSnzy9nmDt3bvkGVgb1MKf95ei33nqrEx988MFpe+LE\niU6f//fzK6+8kvd1l19+eSe2n/utt95y+uzjIKqFpY8AAAAAUIO4UQMAAACAwHCjBgAAAACBoUat\nAOedd17aPuGEE5y+lVZayYn9mrXNNtssbU+bNq0MoytNva99t2vU/Pq1XPyt+v217zUo6hq1FVdc\n0Yk//vjjtH3nnXc6ff6RG5V8rcxCvc/pYp177rlOfM455zjxO++848SnnHJK2g5x6/56qGfJ0jbb\nbJO2n3vuOadv4cKFTnzzzTc7sf/vemiY0z+69tpr07aft/fff9+JN910Uyf+/PPPyzewDNTjnLbr\nwSXpkksuSdtnnHGG0+fXkp966qlp268Z7tixY864WbNmafvrr792+l5++eWmhl121KgBAAAAQA1q\n8kbNGNPJGPOUMWayMWaSMeakhs+3N8aMNsZMbfhvu/IPF+VCnuNBruNAnuNBruNAnuNBrrFYPu+o\nLZR0WpIk3SRtKek4Y0w3SWdJGpMkybqSxjTEqF3kOR7kOg7kOR7kOg7kOR7kGpKKqFEzxoyUdH3D\nx/ZJknxkjOkg6ekkSbo28diqr5PNyv777+/Ed999d86vnz59eto+8cQTnb5HHnkku4EVyV8nW895\n9s9RuvDCC/N+rH3G2k89Vw1YokatnnPtW2op9/9N/eMf/0jb/pkqdm2SJF133XXlG1gZxDSnS7Hx\nxhs78aWXXurE2267bdreZ599nL4Qzlj7qRoHcr3IMccc48SDBg1yYv+sRPtspxdffLF8AysSc/pH\nrVq1StudO3d2+t544w0nfu+995zYPqfr2WefzXpoJYthTi+zzDJp2/97eujQoU783XffpW1/Dvfs\n6Zbcr7LKKk689dZblzLMssunRq1ZU19gM8Z0ltRD0jhJqyZJ8lFD1yxJqzbymAGSBhRyHVQXeY4H\nuY4DeY4HuY4DeY4HuY5b3puJGGNaSxoh6eQkSZwtDZNFb8v95B17kiRDkiTpmeVOcygf8hwPch0H\n8hwPch0H8hwPco28lj4aY5aR9IikJ5Ikuabhc2+rDt5+LVbLli2d+Prrr3di+6113wsvvODEO++8\nsxPPnz+/tMEVIUkSE2ueS1kKWYPb949PkqRnrLn27bHHHml78ODBTp9/xMZGG23kxPZyjBDFPKdL\nsdpqqznxsGHD0vbmm2/u9O21115OXI3t+xcvnSHXTeva1f3WH330USe2j+/Yb7/9nL5QlrmS56bZ\ny5UlaciQIU48d+7ctO0fz2Ef5VMtsc/pddZZx4lPPvnktO0vZezWrZsT+8ud7dfvEGWyPb8xxki6\nTdKbi39RGoySdFhD+zBJI4sZJMJAnuNBruNAnuNBruNAnuNBrrFYPjVq20jqJ+kNY8xrDZ87R9Ll\nku4zxhwpabqk/Rp5PGoDeY4HuY4DeY4HuY4DeY4HuYakPG7UkiR5XlJjb83tmO1wUC3kOR7kOg7k\nOR7kOg7kOR7kGosVvD1/SRersXWyhVh22WWd2N/at3v37o0+9p///KcT77333ml7wYIFGYyuafms\nk81Xred5++23/8m21HT9ml2zFmi92hLb85ei1nNte+ihh5x4t912c+JnnnnGie218FOmTCnfwIrE\nnM7eAw884MT+64MdT5w4sQIjyjbPUly5PvbYY53YrjVfuHCh07fVVls58SuvvFK+gTWCOV2cW265\nxYmPOOKItD158mSnr1evXk7s1ypXAnM6HpnUqAEAAAAAKosbNQAAAAAIDDdqAAAAABAYatTKZIUV\nVnDiP//5z2m7X79+OR979dVXp+2BAwdmO7BGsPY9P6WcuebXrPlnsFUINWqN2GKLLZz4hhtucOJN\nNtnEiSdMmJC2e/ToUb6BFYk5nb2NN97YiR955BEntl/3t9lmG6fPr4Xxa6CKRT1Ldh5++OG0veuu\nuzp9v/3tb534scceq8iYbMzp4vh7CJx99tlp+9xzz3X6nn32WSf+zW9+k7a/+uqrMoxuSczpeFCj\nBgAAAAA1iBs1AAAAAAgMSx8r5IADDkjbd911V86vtZdcnXDCCWUbk40lFcXJailkBZdBsvQxTzvt\ntJMT33vvvU681FI//n8uf0tne1lktTCny2+99dZz4tdffz1tt2jRwunr0KGDE8+ePTuTMbBMKjs9\ne/740jhu3Dinb/jw4U588MEHV2RMNuZ0NuylkHZZiiT179/ficeOHZu2DzroIKdv5syZZRgdczom\nLH0EAAAAgBrEjRoAAAAABIYbNQAAAAAITLNqDyAWEydOTNuff/6509e+fftKDwcZ8WvU7Fqzp556\nqrKDQaaeeOIJJz7//POd+Lrrrkvbq6++utMXQo0aym/KlClOfOihh6bt++67r9LDQYlmzJiRto1x\nS0eGDRtW6eGgTL7++uu0ffvttzt966+/vhP7dahApfGOGgAAAAAEhhs1AAAAAAgMN2oAAAAAEBjO\nUYMkzmeJCOeoRYI5HQfOXIoHczoOzOl4cI4aAAAAANQgbtQAAAAAIDDcqAEAAABAYLhRAwAAAIDA\ncKMGAAAAAIHhRg0AAAAAAtOswtebI2m6pJUa2qEIbTxSZce0ZsbPF2qepfDGVOnxxJLr0MYjMafL\nJbQx1XKepXBzHdp4pNrOdah5lsIbUy3nWQo316GNRwow1xU9Ry29qDEvZ3mWU6lCG48U5pgKFeL3\nENqYQhtPsUL7PkIbjxTmmAoV4vcQ2phCG0+xQvs+QhuPFOaYChXi9xDamEIbT7FC+z5CG48U5phY\n+ggAAAAAgeFGDQAAAAACU60btSFVum5jQhuPFOaYChXi9xDamEIbT7FC+z5CG48U5pgKFeL3ENqY\nQhtPsUL7PkIbjxTmmAoV4vcQ2phCG0+xQvs+QhuPFOCYqlKjBgAAAABoHEsfAQAAACAwFb1RM8b0\nNca8bYyZZow5q5LXtsbwN2PMx8aYidbn2htjRhtjpjb8t10Fx9PJGPOUMWayMWaSMeakao8pC9XO\ndWh5brh+3eW62nluGENQua7HPEvVz3VoeW64ft3lutp5bhhDULmuxzxL1c81ea6Maue5YQzkukgV\nu1EzxiwtabCknSV1k3SgMaZbpa5vGSqpr/e5sySNSZJkXUljGuJKWSjptCRJuknaUtJxDT+Xao6p\nJIHkeqjCyrNUZ7kOJM9SeLmuqzxLweR6qMLKs1RnuQ4kz1J4ua6rPEvB5HqoyHNZBZJniVwXL0mS\ninxI2krSE1Z8tqSzK3V9byydJU204rcldWhod5D0djXG1XD9kZL6hDSmWs11yHmuh1yHkufQc13r\neQ4p1yHnuR5yHUqeQ891rec5pFyT5zjyTK6L/6jk0sfVJc2w4g8aPheCVZMk+aihPUvSqtUYhDGm\ns6QeksaFMqYihZrrYH6mdZLrUPMsBfIzrZM8S+HmOpifaZ3kOtQ8S4H8TOskz1K4uQ7iZ0qeKyKI\nn2vouWYzEU+y6Da64lthGmNaSxoh6eQkSeaFMKZ6Vs2fKbmuLOZ0HJjT8WBOx4E8x4NcN66SN2oz\nJXWy4o4NnwvBbGNMB0lq+O/Hlby4MWYZLfpFuStJkgdDGFOJQs111X+mdZbrUPMsMaezFmquq/4z\nrbNch5qeqJnjAAAgAElEQVRniTmdtVBzTZ6zFWqeJXKdl0reqL0kaV1jzFrGmOaSDpA0qoLXz2WU\npMMa2odp0VrVijDGGEm3SXozSZJrQhhTBkLNdVV/pnWY61DzLDGnsxZqrpnT2Qo1zxJzOmuh5po8\nZyvUPEvkOj8VLtbbRdIUSe9IOrcaRXmS7pH0kaTvtGit7pGSVtSi3V2mSnpSUvsKjqeXFr21OkHS\naw0fu1RzTPWQ69DyXK+5rnaeQ8x1PeY5hFyHlud6zXW18xxirusxzyHkmjzHkWdyXdqHaRgwAAAA\nACAQbCYCAAAAAIHhRg0AAAAAAsONGgAAAAAEhhs1AAAAAAgMN2oAAAAAEBhu1AAAAAAgMNyoAQAA\nAEBguFEDAAAAgMBwowYAAAAAgeFGDQAAAAACw40aAAAAAASGGzUAAAAACAw3agAAAAAQGG7UAAAA\nACAw3KgBAAAAQGC4USuQMeZ4Y8zLxphvjDFDqz0elAd5jgN5jge5jgN5jge5jkPseW5W7QHUoA8l\nXSppJ0ktqzwWlA95jgN5jge5jgN5jge5jkPUeeZGrUBJkjwoScaYnpI6Vnk4KBPyHAfyHA9yHQfy\nHA9yHYfY88zSRwAAAAAIDDdqAAAAABAYbtQAAAAAIDDcqAEAAABAYNhMpEDGmGZa9HNbWtLSxphl\nJS1MkmRhdUeGLJHnOJDneJDrOJDneJDrOMSeZ95RK9x5kr6SdJakQxra51V1RCgH8hwH8hwPch0H\n8hwPch2HqPNskiSp9hgAAAAAABbeUQMAAACAwHCjBgAAAACB4UYNAAAAAAJT0o2aMaavMeZtY8w0\nY8xZWQ0K4SHXcSDP8SDXcSDP8SDXcSDPcSl6MxFjzNKSpkjqI+kDSS9JOjBJksnZDQ8hINdxIM/x\nINdxIM/xINdxIM/xKeUctc0lTUuS5L+SZIwZLml3SY3+shhj2GIyUEmSmBzdBeWaPAdtTpIkKzfS\nx5yuI8zpOGSZ54avIdeBYk7HgTkdjyZyLam0pY+rS5phxR80fA71h1zXj+k5+shzPMh1HMhzPMh1\nHMhzZEp5Ry0vxpgBkgaU+zqoLvIcD3IdB/IcD3IdB/IcD3JdP0q5UZspqZMVd2z4nCNJkiGShki8\n/VrDmsw1ea4LNTmnhw0b5sQbbrhh2u7Tp4/T9+mnn1ZkTDWAOR2HmpzTKApzOg7M6ciUsvTxJUnr\nGmPWMsY0l3SApFHZDAuBIddxIM/xINdxIM/xINdxIM+RKfodtSRJFhpjjpf0hKSlJf0tSZJJmY0M\nwSDXcSDP8SDXcSDP8SDXcSDP8Sl6e/6iLlZjb79uueWWTjx27NhGv/acc85x4iuuuKIsYyqXfHae\nyVet5TlLzZs3T9t33nmn07fvvvs2Go8YMaK8A/vR+CRJemb1ZCHkesaMGU682mqrpe3+/fs7fbff\nfntFxhSCmOf0/vvv78R333132p49e7bTt/POOzvx66+/Xr6BlUGWeZZqL9eVcssttzjx4Ycfnraf\ne+45p++QQw5x4g8//DCTMdT6nL7xxhud+Oijj3Zi++/Rq666yuk788wzyzewwMQ+p9dcc00nHjdu\nXNp+9dVXnT7/9bvWlHvXRwAAAABAGXCjBgAAAACB4UYNAAAAAAJT9nPUaplf+zJt2rS03aVLF6dv\nu+22c2J/ffX333+f8egQol/96ldp269JGzNmjBM/9NBDFRlTvbPXr0vSnnvumbZvuukmp2+ppdz/\nN3XbbbeVb2AI0qqrrurEjzzyiBP37dvXiSdNok4/RhtssEHO2PbLX/7SiQ888EAnvvrqq7MbWA27\n9dZbnbhnT7dcetNNN03bxx57rNPXoUMHJz7ppJPS9ty5c7MaIgJkzI9lXCuttJLT16tXLyd+/vnn\nKzKmSuIdNQAAAAAIDDdqAAAAABAYbtQAAAAAIDDUqOUwc+ZMJ7brWf74xz86fTvttJMTb7HFFk78\nwgsvZDw6hMhfc2978MEHnfiHH34o93Ci8NhjjzmxXaPWrJn7Enf88cc78ahRo9L2J598UobRoRoe\nffRRJ37yySfTdu/evZ0++9w9yf39kahRi8mwYcPSdrdu3Zy+TTbZpNHHvfjii048cuTIbAdWJ8aP\nH+/E9vmGkrT++uun7VatWjl9Bx98sBPffPPNaTvXGbeoPdOnT3fi888/P237Z/H58YYbbli+gVUJ\n76gBAAAAQGC4UQMAAACAwJgkSSp3MWMqd7EysJfIPP30007fOuus48TvvvuuE9vb93700UfZD65E\nSZKYpr8qP7We51L85z//Sdubb76507fKKqs48Zw5cyoyJs/4JEkaX59ZoBBy7S+R+fvf/562d9tt\nt5yPtZck77333k7fxx9/nMHoqoc5/aN27dql7abm3VNPPeXE/lLJ0GSZZ6n2c12K2bNnp+327dvn\n/Th7CbW05GtJVup9Tttb8g8ePDjn11588cU/2a4HzGnXtttum7b9v739e5gLLrjAiQcNGlS2cWUh\nn1zzjhoAAAAABIYbNQAAAAAIDDdqAAAAABAYtucvwIcffpi233rrLafPr1Fba621nHjNNddM2yHW\nqCEbxvy43NivdalSTVrd+/LLL524X79+afu1115z+vx5ufXWW6dtv67E3/YXtWv+/Plpe8iQIU7f\ngAEDnLhXr15OvP3226dtvz4CQHaGDx+etv2/qU455RQn7t+/f9q+9957nT7/7zPUtmeffTZtv/nm\nm05f165dnbhHjx5OvNxyy6XtBQsWlGF05cc7agAAAAAQGG7UAAAAACAw3KgBAAAAQGDqvkbNPwtl\nmWWWceIWLVqk7ffffz/v573iiiuceMcdd3TiZZdd1olPPPHEtG2ftYXa1qVLFyfebLPN0vZdd91V\n6eFAbj3Sqaee6vQNGzbMidu2bZu2r7zySqfv66+/duLbb789qyGiwr777ru0fdlllzl9/ll7P/vZ\nz5z4rLPOStt2rYQk/fDDD1kNEWXgvz6fccYZOb/ermdB5c2dOzdt2/VqkrTPPvs48RprrJG2/bOy\nDjnkECf+6quvshoiqmyXXXZx4hdffNGJ99prr0Yf6/8O1QreUQMAAACAwHCjBgAAAACBqYulj507\nd3Zie2lL9+7dnb42bdo48QorrJC2H3/8cafPX+o0evTotD127Finb8KECU68+eabO/FWW22Vtv2l\nNbNmzRJq07777lvtISCHUaNGObG9jE2SbrjhhrTtL3vyl00++uijTvzxxx9nMUQ0olOnTk684oor\nOvERRxyRtlu1apXzub755pu0fcsttzh9n376qRP7r899+vRJ2/4W4VdffXXO6yJ7zZs3d+KBAwc6\n8f7775+2/d8L/3eqFPYyPbu0AS47BxtuuKHT5y9F22GHHdL2pEmTnD77eCTJXfq4xx57OH3PPfec\nE++6665OzBFJ5bHnnnum7UsvvdTp87fRt48ymjx5stP38MMPO7H9mj19+nSn75hjjnHiBx54wInt\n3w3/KBb/qJZQ8Y4aAAAAAASGGzUAAAAACAw3agAAAAAQGJMkSeUuZkwmF1t66aWdeOjQoU580EEH\nZXEZffbZZ078zDPPpO0777zT6evdu7cT//73v2/0ec877zwn/uMf/1jsEDOTJIlp+qvyk1Wea8FF\nF13kxBdccEHa9rfn79evXyWG1JTxSZL0zOrJai3X7dq1c+Lnn38+ba+//vo5H/vUU085sf06E2K9\nWi3OaTsHY8aMcfr82rFqmD17thOvvfbaTuwf6VAJWeZZCnNO/+pXv/rJtiSdffbZjT5uqaXc/xed\n5XEK8+bNS9s77bST0/fyyy9ndh1bLc7pwYMHp+1jjz22Epd0alKlJf9GrNQ4ilUrc/qll15yYvv1\n268P9e817Bq1XH2S9Mknn6Tt7bbbLueY/Ho3+7n91w7/uJVqyCfXvKMGAAAAAIHhRg0AAAAAAtPk\njZox5m/GmI+NMROtz7U3xow2xkxt+G+7XM+B2kCu40Ce40Gu40Ce40Gu40CesViTNWrGmG0lzZd0\nR5IkGzR87kpJnyVJcrkx5ixJ7ZIkObPJi2W0Tvaoo45y4ptuuqnRr3333Xed+IsvvnDijTbaKIsh\nFcSvbzv88MMrPgZfkiQmq1yHWONQLrlq1C688EKn75JLLqnEkJoyXtKpCmxOV8vRRx+dtu0z1fJh\nnxnjn9cWglqY0xtssIETP/bYY2l7tdVWy/lY+7W8qdoju+ahbdu2hQwxp/fee8+Jf/Ob36Ttt956\nK7Pr5JJlnhseV/U57Z+vZdc5tW/fPu/nef/99504198Kkls/7p+rmIs///fee++8H1uIWpjT/plm\nI0aMSNv+GVf+mYZ2DbH/M/z5z3/uxPaZbIXutfCnP/0pbdv/ZkvSt99+W9BzlUOtzOnvv//eie08\nzJgxw+mbM2dOo8/j14f7c89+Xr9+zc/9YYcd5sTDhg1r9LH+Wal/+ctfGh1juWRSo5YkybOSPvM+\nvbukxd/9MEl7CDWPXMeBPMeDXMeBPMeDXMeBPGOxZkU+btUkSRYf7T5L0qqNfaExZoCkAY31I3h5\n5Zo81zzmdDyY03FgTseDOR0H5nSEir1RSyWL3qNt9G3VJEmGSBoiZff26zrrrJOz/7XXXkvbu+22\nm9P3+eefO/HWW2+dtv3lFptttpkTb7zxxgWNszGrrLKKEy+77LJOXI0tnvORK9flyHOW9t1337Td\nokWLnF9rL5cdO3ZsQdexl02EuBwuH9WY04Xwt2n3t0i3lzOssMIKOZ+rkGVUPvsIDv/35NNPPy36\neSup2nP6qquucmJ7ueOCBQucvkGDBjnxdddd1+jX+po3b562999/f6fPfm2QpL59+zqxfxyMrXPn\nzk68/fbbp+1KLX3MR4hzes0110zbfjlAly5dnHillVYq6hr28mRJmjBhQs6vHzhwYNpu3bp13tfx\nl1RVU7XntF9uYrv11lud+Mknn2z0a++//34nbtmypRN37do1bd94441On1/S4v+NZefZL4fxX2dC\nFcKczvV77x87NWTIkEa/1l/66L+u2kdw/PKXv8w5Jv+1JNeyyXPOOceJn3jiibQd0ut3sbs+zjbG\ndJCkhv+Gd4gQskKu40Ce40Gu40Ce40Gu40CeI1TsjdooSYsr9g6TNDKb4SBA5DoO5Dke5DoO5Dke\n5DoO5DlC+WzPf4+kf0vqaoz5wBhzpKTLJfUxxkyV1LshRo0j13Egz/Eg13Egz/Eg13Egz1isye35\nM71YRutkL7/c/d0844wznPj4449P2/7a5UL42zjvvvvuafvPf/6z09dULUwuO+ywgxM/88wzRT9X\nsfLZIjRflapxsOtO/HXJ++23nxMX+3vu1x7523HbWwRL0lJL/fj/Pqpx9EMexidJ0jOrJ6tGjdoL\nL7zgxFtssUWlh7AEv9bitNNOc+KJEyeq0mphTj/++ONO3KdPn7T94IMPOn1+LVm5HHfccU58+umn\np+011lgj52PtepcDDzzQ6bOPHshSlnmWKjen7e3Xhw8fnvNrv/vuu7T98ssvO33+v/H33HNP0WOa\nPXt22i6kfrWS2/Nn9VzlynP37t2d+JVXXknb/hEop5xySjmGsMS/Cf7rzPLLL5+2v/zyS6fvsssu\nc2K/1qoSQp3TvXr1cmL/b9VPPvkkbdv1ulJ2NV/2cQ+S+3e5lHv7/qa29rfrznPV1GUpk+35AQAA\nAACVxY0aAAAAAASGGzUAAAAACEzJ56iFqKlz1vI1b948J7broDbYYAOnr3///k5cSM2avz7ffu5a\nOY+pGuyzNvz6lXfeeceJ7fqQqVOn5nzeo48+Om1vs802Tp8f++yznVC8lVde2Ymvv/76tL3ppptW\nejhN6t27txP/+9//duIBA348d7SUGppa55+Hleu1evTo0eUezk8aPHiwE9tn6/zhD39w+vwz2dq0\nadPo1z733HNOPH/+/JLGGbrNN9/cif3zjy644IK8n+umm25K2/Y5iVmz56Zfq+j76quv0natnplZ\nDpMmTXLiN998M2337JlZeXRO48aNc2K/bs6ei2uttZbT5/9e2n8vPPDAA1kNsSa9/fbbTuzXfNnx\nz3/+c6cvqxq1Qw891In9M9j8s9H22GOPnxzfTznqqKPStl8jPWfOnILGmSXeUQMAAACAwHCjBgAA\nAACBqcmlj/7brz57e/7nn3/e6Xv44YczGcOZZ57pxP6yljvuuMOJ7e1gfaussooT20sfq7FVf63I\ntQxxr732cuIJEybk/byfffZZ2vbzeO211zrxiiuu6MT+1tEozhFHHOHE++yzT1HPY2+XLkkfffRR\n0WPy2Vt5+68rV199tRPbW/36x3HYyy3qnb/l+dprr12lkeRv2rRpafuggw5y+nbddVcnXm655dK2\nv0TXXzbVt2/frIYYpO22286J/W3Pc7GXOkuFLZMsxfnnn5+2/WVS9tbdknTSSSel7WHDhpV3YDXs\n73//e9q+4oornL57773Xifv165e2v/3228zG8OGHHzqxvQzXP1qla9euTpxre/7YlkLa2+9LS25h\nb5cAXXrppU6fvYRckhYsWFDUGPzjFMaPH+/E/tEY5557btr2l6P72/Pbr9lPPfWU0+cfxVRJvKMG\nAAAAAIHhRg0AAAAAAsONGgAAAAAExvhrNMt6MWMyuVjLli2d+NVXX3XiddddN21PnjzZ6Rs4cKAT\n29u2Z6lLly5O/I9//CNtN3V8wD//+c+0ffbZZzt9r732WgajW1KSJLn3LS1AVnluil0f5q+dtrdk\nlYrfPtn//fCPXdhqq62Ket4qGp8kSWZ7JJcr1/48zVUnkMuBBx7oxPfdd1/RY8rF3yLY36LaNnfu\nXCf267T8Y0GKFeKcXm+99ZzY3rrbd+yxxzqxXw8RgtNPP92J/Rocm19zY7+2+DWZ//vf//IeQ5Z5\nlkrLtb0lv/3vmCS1atWq0cfdfffdTmwfkSJJX3/9dbFDKtqyyy7rxK1bt3Ziex5///33FRlTiHO6\nKfa/0/6eAX492F133ZW27SNNJPc4hCytscYaTvzoo486cbdu3dL29OnTnT6/DnPGjBmZjCmkOZ2L\nX2dr/+z8Gs9BgwY5sV0PWin+/YD/+2eP2b83WnrppcsypnxyzTtqAAAAABAYbtQAAAAAIDDcqAEA\nAABAYGryHDV/rfJvf/tbJ7bPWbPXF0vSQw895MTXXHNN2r7xxhudvlLWG9tn70ju+VpN1aj9+te/\nTtv2mn9pyVqdq666qtgh1jz7jCz/LI1jjjnGiYutUWvbtq0T++uuUR7+WSl23Zafk1z889emTJni\nxO+//37ats/PK9SsWbOceNy4cU68xRZbpO127do5ff4ZbP45a7HKVdMUCvvfD0nq0aNH2j7ggAOc\nvubNmzvx7rvvnrZXXnllp6+QGrWQLLPMMmm7kPx16tTJiTt27OjE/r+n5WLXtNqvDZI0duzYioyh\n3nz66adp+4QTTnD6/HPU/HMKbfb5uFJ2c8TP8w033ODE9hzv3Lmz0+efnxfba/fjjz/uxPZ5h/6Z\nZf6+ASuttFLa9v+u9XOSFb+m0N47QnLPUfNr1Pw69Lfeeivj0TWOd9QAAAAAIDDcqAEAAABAYLhR\nAwAAAIDA1OQ5aj7/fJObb745bft1Arn4a5OvvPJKJy6lZs1er3/HHXc4ff7a3Vz8MfnnrBWrFs9n\nsd16661O/Lvf/c6J7fXQhdQi/fDDD07s1zjuvffeeT9XIGriHDXfRhttlLZHjx7t9Nm5LZQ9p/3z\nc/x18v7vWK4xjBw50ontsx19dq2ltORZfcUKcU63adPGiZ955hkn3njjjdP2e++95/T17On+2vrn\n0YXArjWz65KlJeuubP55PoXUZIV05tI222yTtp9++umix/Dcc8858dSpU9P2hAkTnL7Bgwfn/bz+\nuU977rmnE9s1Uv7898/1qkbNWohzuhT2GWuSu0+AX188c+ZMJ7bPHnzyySfLMLpFnnjiibRt7x8g\nLXmurV2jWoqQ5nSxRowY4cT+37l2vb9/nqb/d1W56sH8c3Lt1wf/bz/OUQMAAAAApLhRAwAAAIDA\n1MXSR5/9FqW/5Mhfurbeeus1+jxvvPGGE1944YVp21/aVAh7q25JOu+885x4l112afSxLH38af6S\nFn/b1YMPPjht33PPPTmf6/LLL0/bAwcOdPpOOukkJ/7rX/9a0DgDUJNLH232MkhJOu2005zYXmKx\n3HLLOX1LLRXe/5uKaemjzz9e5JRTTmn0a/2lkFtttVXa/vjjjzMdV778Lejt14szzjjD6WvRooUT\nDxkyJG2ffPLJTt8333yT9xhCWiZlzzf730tJOvXUU4sflGX+/PlO/MEHHzT6tf5894/G8I9FyOWd\nd95x4j59+qTtUsoiClELczor++67rxP7fydtuOGGadtfZjxo0CAnLuXvtX79+qVtfzt+/+/n4447\nLm3fdNNNRV8zpDldLH9uXX311U5s/03m/xz9Y5AmT56ctu0jAKQl/6bPZaeddnJiP5/2mP3n9Zfi\nZoWljwAAAABQg7hRAwAAAIDAcKMGAAAAAIGpyxq1XOztnyXplVdeyfuxCxcuTNvz5s3L+bX+dqL2\nNrT+2l2/JsVeV//qq686fb1793bizz//POc48lVva99ff/11J7ZrE375y186fdOnT3diu77Nr32j\nRs0VQq5zOeqoo5zYrmuS3O24mzVr5vT56+TL5f7773fiQo4UyaUW5rRfQ2TXDNi5+Sn267G/Pbdd\n05Clbt26ObH/emz/Dvm1VH7Nmn3cg78VdCFCrWfxt76/7rrrnPhnP/tZFpfJyf/9KuXn7L+WDB06\ntOjnKlYtzOly6d69uxOPGjUqbXfu3Nnp8/Ps16U/+OCDafu///2v02cfBSFJ/fv3T9t/+ctfnD7/\n72e77mr48OEqVqhzuhTbbrutE9t/S+Xaul9yf84LFixw+grZuv8Xv/hFo88rSW+//Xba9o+D8a+b\nFWrUAAAAAKAGNXmjZozpZIx5yhgz2RgzyRhzUsPn2xtjRhtjpjb8t11Tz4Vwked4kOs4kOd4kOs4\nkOd4kGssls87agslnZYkSTdJW0o6zhjTTdJZksYkSbKupDENMWoXeY4HuY4DeY4HuY4DeY4HuYak\nImrUjDEjJV3f8LF9kiQfGWM6SHo6SZKuTTy26utk/TXr9trmxx57zOnr0qVLJYaUk1+v4tezZMVf\nJ1vrefbPl7vkkkvS9gsvvOD0LbPMMk7cteuP3959993n9F100UVOPGvWrFKGWQ1L1KjVeq6zcsQR\nRzhxx44dc369XUO1/vrr5/xau6b1D3/4g9Pnny+TlVqc0/Zr7u233+70bb311pUYQkns2li/hsmv\n0crKT9U4hJhrv2Ztyy23TNtZnbHmK6VGzT+by69RmzhxYvEDK1ItzulyadOmTdq+7LLLnD7/DDZ/\nXwC7Bsr/G3jKlClO3KFDh7Tdtm1bp2/cuHFOvMMOO6Ttr776qtGxN6VW5nRW/NeGvfbay4lznY3a\n1Blsdn+uPsmtZbTPzyunfGrUmjX1BTZjTGdJPSSNk7RqkiQfNXTNkrRqI48ZIGlAIddBdZHneJDr\nOJDneJDrOJDneJDruOW9mYgxprWkEZJOTpLE2fIwWXRb+pN37EmSDEmSpGeWO82hfMhzPMh1HMhz\nPMh1HMhzPMg18lr6aIxZRtIjkp5IkuSahs+9rTp7+9VfAtepUycnvvDCC9P2oYcemtl1/SUxF198\ncdqeO3eu01eu4xSSJDH1nOfjjz8+bfs/b3tLVkmaM2dO2va38q8D45Mk6VnPucYitT6nW7Ro4cRr\nrrmmE9tbLfvHLhRiv/32c+JVV3X/B/WHH36YtkeMGOH03XzzzU78/vvvp+0vvvii6DEVYvHSmVrL\ntb2EqUePHnk/rlevXk7sL3uzNbX00V8iP3bs2LQ9e/Zsp2/atGl5j7Fcan1OF2LAAPfNoCFDhuT9\nWP+1w1/Wbm+jv+mmmzp9LVu2bPR5H3jgASf2l8d98803eY8xl1qd0+ViL40cNGiQ02eXqkiFLX30\nn+v8888vaZzFyGR7frPoO7tN0puLf1EajJJ0WEP7MEkjixkkwkCe40Gu40Ce40Gu40Ce40GusVg+\nNWrbSOon6Q1jzGsNnztH0uWS7jPGHClpuqT9Gnk8agN5jge5jgN5jge5jgN5jge5hqQ8btSSJHle\nUmNvze2Y7XBQLeQ5HuQ6DuQ5HuQ6DuQ5HuQaixW8PX9JF6vxdbL1LJ91svkiz0FbYnv+UpDrcDGn\n45BlniVyHTLmdByY0/HIpEYNAAAAAFBZ3KgBAAAAQGC4UQMAAACAwHCjBgAAAACB4UYNAAAAAALD\njRoAAAAABIYbNQAAAAAIDDdqAAAAABAYbtQAAAAAIDDcqAEAAABAYLhRAwAAAIDAcKMGAAAAAIHh\nRg0AAAAAAsONGgAAAAAEhhs1AAAAAAgMN2oAAAAAEBhu1AAAAAAgMNyoAQAAAEBguFEDAAAAgMBw\nowYAAAAAgeFGDQAAAAAC06zC15sjabqklRraoQhtPFJlx7Rmxs8Xap6l8MZU6fHEkuvQxiMxp8sl\ntDHVcp6lcHMd2nik2s51qHmWwhtTLedZCjfXoY1HCjDXJkmScg9kyYsa83KSJD0rfuFGhDYeKcwx\nFSrE7yG0MYU2nmKF9n2ENh4pzDEVKsTvIbQxhTaeYoX2fYQ2HinMMRUqxO8htDGFNp5ihfZ9hDYe\nKcwxsfQRAAAAAALDjRoAAAAABKZaN2pDqnTdxoQ2HinMMRUqxO8htDGFNp5ihfZ9hDYeKcwxFSrE\n7yG0MYU2nmKF9n2ENh4pzDEVKsTvIbQxhTaeYoX2fYQ2HinAMVWlRg0AAAAA0DiWPgIAAABAYLhR\nAwAAAIDAVPRGzRjT1xjztjFmmjHmrEpe2xrD34wxHxtjJlqfa2+MGW2Mmdrw33YVHE8nY8xTxpjJ\nxnm5dZgAAALtSURBVJhJxpiTqj2mLFQ716HlueH6dZfraue5YQxB5boe8yxVP9eh5bnh+nWX62rn\nuWEMQeW6HvMsVT/X5Lkyqp3nhjGQ6yJV7EbNGLO0pMGSdpbUTdKBxphulbq+Zaikvt7nzpI0JkmS\ndSWNaYgrZaGk05Ik6SZpS0nHNfxcqjmmkgSS66EKK89SneU6kDxL4eW6rvIsBZProQorz1Kd5TqQ\nPEvh5bqu8iwFk+uhIs9lFUieJXJdvCRJKvIhaStJT1jx2ZLOrtT1vbF0ljTRit+W1KGh3UHS29UY\nV8P1R0rqE9KYajXXIee5HnIdSp5Dz3Wt5zmkXIec53rIdSh5Dj3XtZ7nkHJNnuPIM7ku/qOSSx9X\nlzTDij9o+FwIVk2S5KOG9ixJq1ZjEMaYzpJ6SBoXypiKFGqug/mZ1kmuQ82zFMjPtE7yLIWb62B+\npnWS61DzLAXyM62TPEvh5jqInyl5roggfq6h55rNRDzJotvoip9ZYIxpLWmEpJOTJJkXwpjqWTV/\npuS6spjTcWBOx4M5HQfyHA9y3bhK3qjNlNTJijs2fC4Es40xHSSp4b8fV/LixphltOgX5a4kSR4M\nYUwlCjXXVf+Z1lmuQ82zxJzOWqi5rvrPtM5yHWqeJeZ01kLNNXnOVqh5lsh1Xip5o/aSpHWNMWsZ\nY5pLOkDSqApeP5dRkg5raB+mRWtVK8IYYyTdJunNJEmuCWFMGQg111X9mdZhrkPNs8SczlqouWZO\nZyvUPEvM6ayFmmvynK1Q8yyR6/xUuFhvF0lTJL0j6dxqFOVJukfSR5K+06K1ukdKWlGLdneZKulJ\nSe0rOJ5eWvTW6gRJrzV87FLNMdVDrkPLc73mutp5DjHX9ZjnEHIdWp7rNdfVznOIua7HPIeQa/Ic\nR57JdWkfpmHAAAAAAIBAsJkIAAAAAASGGzUAAAAACAw3agAAAAAQGG7UAAAAACAw3KgBAAAAQGC4\nUQMAAACAwHCjBgAAAACB+X/KFlG5pkb1kQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x261cdab1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate testing input as flipping upside down images to training data\n",
    "y1hot_flip = np.copy(y1hot)\n",
    "y_flip = np.copy(y)\n",
    "def flip(img):\n",
    "    rtnimg = np.array([img[img_height-1-i,...] for i in range(img_height)])\n",
    "    return rtnimg\n",
    "x_flip = list(map(lambda img: flip(img),x))\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in np.arange(2*7):\n",
    "    plt.subplot(2,7,i+1)\n",
    "    plt.imshow(x_flip[5411+i][...,0],cmap='gray')\n",
    "    plt.title(y_flip[5411+i][0])\n",
    "    \n",
    "x_rest_flip = list(map(lambda img: flip(img),x_rest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN Graph Construction\n",
    "\n",
    "1. Define the input output tensors\n",
    "2. Define the graph and construct it\n",
    "3. Define the loss and optimizer\n",
    "\n",
    "### First define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "EPOCHS = 3\n",
    "EPOCHS_TF = 10\n",
    "BATCH_SIZE = 128\n",
    "rate = 0.001\n",
    "drop_out_keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using one-hot decoding\n",
    "cNum = 1\n",
    "x_feed = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))\n",
    "one_hot_y_feed = tf.placeholder(tf.int32, (None, n_classes))\n",
    "\n",
    "x_feed_student = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))\n",
    "one_hot_y_feed_student = tf.placeholder(tf.int32, (None, n_classes))\n",
    "#one_hot_y = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the graph and construct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kSize = 5\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "layer_depth = {\n",
    "    'layer_1': 16,\n",
    "    'layer_2': 32,\n",
    "    'fully_connected_1': 256,\n",
    "    'fully_connected_2': 128,\n",
    "    'out': n_classes,\n",
    "}\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 has shape= (?, 24, 24, 16)\n",
      "conv2 has shape= (?, 8, 8, 32)\n",
      "flatten(conv2) has shape= (?, 512)\n",
      "logits has shape= (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# CNN architecture\n",
    "\n",
    "with tf.variable_scope('teacher'):\n",
    "    weights = {\n",
    "        'layer_1': tf.get_variable('layer_1w', shape=(kSize, kSize, cNum, layer_depth['layer_1']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'layer_2': tf.get_variable('layer_2w', shape=(kSize, kSize, layer_depth['layer_1'], layer_depth['layer_2']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'fully_connected_1': tf.get_variable('fully_connected_1w', shape=(4*4*layer_depth['layer_2'], layer_depth['fully_connected_1']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'fully_connected_2': tf.get_variable('fully_connected_2w', shape=(layer_depth['fully_connected_1'], layer_depth['fully_connected_2']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'out': tf.get_variable('outw', shape=(layer_depth['fully_connected_2'], layer_depth['out']),initializer=tf.random_normal_initializer(mu,sigma))\n",
    "    }\n",
    "    #+0.3 over the first epochs than +-0\n",
    "\n",
    "    biases = {\n",
    "        'layer_1': tf.get_variable('layer_1b', shape=(layer_depth['layer_1']),initializer=tf.zeros_initializer),\n",
    "        'layer_2': tf.get_variable('layer_2b', shape=(layer_depth['layer_2']),initializer=tf.zeros_initializer),\n",
    "        'fully_connected_1': tf.get_variable('fully_connected_1b', shape=(layer_depth['fully_connected_1']),initializer=tf.zeros_initializer),\n",
    "        'fully_connected_2': tf.get_variable('fully_connected_2b', shape=(layer_depth['fully_connected_2']),initializer=tf.zeros_initializer),\n",
    "        'out': tf.get_variable('outb', shape=(layer_depth['out']),initializer=tf.zeros_initializer)\n",
    "    }\n",
    "\n",
    "def CNNMNIST_teacher(x):\n",
    "    # Layer 1: Convolutional. Input = 28x28xcNum. Output = 24x24xlayer_depth['layer_1'].\n",
    "    conv1   = tf.nn.conv2d(x, weights['layer_1'], strides=[1, 1, 1, 1], padding='VALID') + biases['layer_1']\n",
    "    print('conv1 has shape=',conv1.shape)\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1'].\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 8x8xlayer_depth['layer_2'].\n",
    "    conv2   = tf.nn.conv2d(conv1, weights['layer_2'], strides=[1, 1, 1, 1], padding='VALID') + biases['layer_2']\n",
    "    print('conv2 has shape=',conv2.shape)\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 8x8xlayer_depth['layer_2']. Output = 4x4xlayer_depth['layer_2'].\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 4x4xlayer_depth['layer_2']. Output = 4x4xlayer_depth['layer_2'].\n",
    "    print('flatten(conv2) has shape=',flatten(conv2).shape)\n",
    "    fc0 = flatten(conv2)\n",
    "\n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 5x5xlayer_depth['layer_2']. Output = layer_depth['fully_connected_1'].\n",
    "    fc1   = tf.matmul(fc0, weights['fully_connected_1']) + biases['fully_connected_1']\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)    # dropout\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = layer_depth['fully_connected_1']. Output = layer_depth['fully_connected_2'].\n",
    "    fc2    = tf.matmul(fc1, weights['fully_connected_2']) + biases['fully_connected_2']\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)    # dropout\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = layer_depth['fully_connected_2']. Output = layer_depth['out'].\n",
    "    logits = tf.matmul(fc2, weights['out']) + biases['out']\n",
    "    print('logits has shape=',logits.shape)\n",
    "    return logits\n",
    "\n",
    "logits = CNNMNIST_teacher(x_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 has shape= (?, 24, 24, 16)\n",
      "conv2 has shape= (?, 8, 8, 32)\n",
      "flatten(conv2) has shape= (?, 512)\n",
      "logits has shape= (?, 2)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('student'):\n",
    "    weights_student = {\n",
    "        'layer_1': tf.get_variable('layer_1w', shape=(kSize, kSize, cNum, layer_depth['layer_1']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'layer_2': tf.get_variable('layer_2w', shape=(kSize, kSize, layer_depth['layer_1'], layer_depth['layer_2']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'fully_connected_1': tf.get_variable('fully_connected_1w', shape=(4*4*layer_depth['layer_2'], layer_depth['fully_connected_1']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'fully_connected_2': tf.get_variable('fully_connected_2w', shape=(layer_depth['fully_connected_1'], layer_depth['fully_connected_2']),initializer=tf.random_normal_initializer(mu,sigma)),\n",
    "        'out': tf.get_variable('outw', shape=(layer_depth['fully_connected_2'], layer_depth['out']),initializer=tf.random_normal_initializer(mu,sigma))\n",
    "    }\n",
    "    #+0.3 over the first epochs than +-0\n",
    "\n",
    "    biases_student = {\n",
    "        'layer_1': tf.get_variable('layer_1b', shape=(layer_depth['layer_1']),initializer=tf.zeros_initializer),\n",
    "        'layer_2': tf.get_variable('layer_2b', shape=(layer_depth['layer_2']),initializer=tf.zeros_initializer),\n",
    "        'fully_connected_1': tf.get_variable('fully_connected_1b', shape=(layer_depth['fully_connected_1']),initializer=tf.zeros_initializer),\n",
    "        'fully_connected_2': tf.get_variable('fully_connected_2b', shape=(layer_depth['fully_connected_2']),initializer=tf.zeros_initializer),\n",
    "        'out': tf.get_variable('outb', shape=(layer_depth['out']),initializer=tf.zeros_initializer)\n",
    "    }\n",
    "\n",
    "def CNNMNIST_student(x):\n",
    "    # Layer 1: Convolutional. Input = 28x28xcNum. Output = 24x24xlayer_depth['layer_1'].\n",
    "    conv1   = tf.nn.conv2d(x, weights_student['layer_1'], strides=[1, 1, 1, 1], padding='VALID') + biases_student['layer_1']\n",
    "    print('conv1 has shape=',conv1.shape)\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1'].\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 8x8xlayer_depth['layer_2'].\n",
    "    conv2   = tf.nn.conv2d(conv1, weights_student['layer_2'], strides=[1, 1, 1, 1], padding='VALID') + biases_student['layer_2']\n",
    "    print('conv2 has shape=',conv2.shape)\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 8x8xlayer_depth['layer_2']. Output = 4x4xlayer_depth['layer_2'].\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 4x4xlayer_depth['layer_2']. Output = 4x4xlayer_depth['layer_2'].\n",
    "    print('flatten(conv2) has shape=',flatten(conv2).shape)\n",
    "    fc0 = flatten(conv2)\n",
    "\n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 5x5xlayer_depth['layer_2']. Output = layer_depth['fully_connected_1'].\n",
    "    fc1   = tf.matmul(fc0, weights_student['fully_connected_1']) + biases_student['fully_connected_1']\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)    # dropout\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = layer_depth['fully_connected_1']. Output = layer_depth['fully_connected_2'].\n",
    "    fc2    = tf.matmul(fc1, weights_student['fully_connected_2']) + biases_student['fully_connected_2']\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)    # dropout\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = layer_depth['fully_connected_2']. Output = layer_depth['out'].\n",
    "    logits = tf.matmul(fc2, weights_student['out']) + biases_student['out']\n",
    "    print('logits has shape=',logits.shape)\n",
    "    return logits\n",
    "\n",
    "logits_student = CNNMNIST_student(x_feed_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Scope: len(trainable_collection_t)=10; len(global_collection_t)=10\n",
      "Without Scope: len(trainable_collection_s)=10; len(global_collection_s)=10\n",
      "Without Scope: len(trainable_collection)=20; len(global_collection)=20\n"
     ]
    }
   ],
   "source": [
    "trainable_collection_t = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='teacher')\n",
    "global_collection_t = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='teacher')\n",
    "print('Without Scope: len(trainable_collection_t)={}; len(global_collection_t)={}'.format(len(trainable_collection_t),len(global_collection_t)))\n",
    "\n",
    "trainable_collection_s = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='student')\n",
    "global_collection_s = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='student')\n",
    "print('Without Scope: len(trainable_collection_s)={}; len(global_collection_s)={}'.format(len(trainable_collection_s),len(global_collection_s)))\n",
    "\n",
    "trainable_collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "global_collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "print('Without Scope: len(trainable_collection)={}; len(global_collection)={}'.format(len(trainable_collection),len(global_collection)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'teacher/layer_1w:0' shape=(5, 5, 1, 16) dtype=float32_ref>, <tf.Variable 'teacher/layer_2w:0' shape=(5, 5, 16, 32) dtype=float32_ref>, <tf.Variable 'teacher/fully_connected_1w:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'teacher/fully_connected_2w:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'teacher/outw:0' shape=(128, 2) dtype=float32_ref>, <tf.Variable 'teacher/layer_1b:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'teacher/layer_2b:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'teacher/fully_connected_1b:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'teacher/fully_connected_2b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'teacher/outb:0' shape=(2,) dtype=float32_ref>]\n",
      "\n",
      "\n",
      "[<tf.Variable 'student/layer_1w:0' shape=(5, 5, 1, 16) dtype=float32_ref>, <tf.Variable 'student/layer_2w:0' shape=(5, 5, 16, 32) dtype=float32_ref>, <tf.Variable 'student/fully_connected_1w:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'student/fully_connected_2w:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'student/outw:0' shape=(128, 2) dtype=float32_ref>, <tf.Variable 'student/layer_1b:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'student/layer_2b:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'student/fully_connected_1b:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'student/fully_connected_2b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'student/outb:0' shape=(2,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(global_collection_t)\n",
    "print('\\n')\n",
    "print(global_collection_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss, optimizer\n",
    "# softmax_cross_entropy_with_logits: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "# labels: Each row labels[i] must be a valid probability distribution.\n",
    "# logits: Unscaled log probabilities.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y_feed,logits=logits)\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "loss_tf_op = tf.nn.l2_loss(logits_student-logits)\n",
    "optimizer_tf = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_tf_operation = optimizer_tf.minimize(loss_tf_op, var_list = trainable_collection_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy evaluation\n",
    "# calculate the average accuracy by calling evaluate(X_data, y_data)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y_feed, axis=1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    assert(len(X_data)==len(y_data))\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x_feed: batch_x, one_hot_y_feed: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "correct_prediction_student = tf.equal(tf.argmax(logits_student, axis=1), tf.argmax(one_hot_y_feed_student, axis=1))\n",
    "accuracy_operation_student = tf.reduce_mean(tf.cast(correct_prediction_student, tf.float32))\n",
    "def evaluate_student(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    assert(len(X_data)==len(y_data))\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation_student, feed_dict={x_feed_student: batch_x, one_hot_y_feed_student: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10871, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y1hot_flip.shape)\n",
    "type(y1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Train Accuracy = 0.967; Flip Accuracy = 0.098\n",
      "\n",
      "EPOCH 2 ...\n",
      "Train Accuracy = 0.999; Flip Accuracy = 0.151\n",
      "\n",
      "EPOCH 3 ...\n",
      "Train Accuracy = 0.999; Flip Accuracy = 0.110\n",
      "\n",
      "Model saved\n",
      "Before assignment\n",
      "After assignment\n",
      "EPOCH 1 ...\n",
      "Acc loss = 3.871242271944786\n",
      "Train Accuracy = 0.156; Flip Accuracy = 0.998\n",
      "\n",
      "EPOCH 2 ...\n",
      "Acc loss = 0.40557907682784994\n",
      "Train Accuracy = 0.101; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 3 ...\n",
      "Acc loss = 0.3005437100890939\n",
      "Train Accuracy = 0.103; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 4 ...\n",
      "Acc loss = 0.2651689475203995\n",
      "Train Accuracy = 0.097; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 5 ...\n",
      "Acc loss = 0.23342516055794454\n",
      "Train Accuracy = 0.116; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 6 ...\n",
      "Acc loss = 0.21965102370403122\n",
      "Train Accuracy = 0.114; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 7 ...\n",
      "Acc loss = 0.20624420208826136\n",
      "Train Accuracy = 0.092; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 8 ...\n",
      "Acc loss = 0.19072401081668058\n",
      "Train Accuracy = 0.099; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 9 ...\n",
      "Acc loss = 0.17994849463196053\n",
      "Train Accuracy = 0.100; Flip Accuracy = 0.999\n",
      "\n",
      "EPOCH 10 ...\n",
      "Acc loss = 0.17795381913594363\n",
      "Train Accuracy = 0.106; Flip Accuracy = 0.999\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "# mini-batch Adam training, will save model as ./models/mnist-cnn-model\n",
    "if not os.path.isdir('./models'):\n",
    "    os.makedirs('./models')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = x_num\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    train_accuracy = np.zeros(EPOCHS)\n",
    "    flip_accuracy = np.zeros(EPOCHS)\n",
    "    for i in range(EPOCHS):\n",
    "        acc_train_accuracy = 0\n",
    "        X_train, y_train = shuffle(x, y1hot)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x_feed: batch_x, one_hot_y_feed: batch_y, keep_prob: drop_out_keep_prob})\n",
    "            acc_train_accuracy += evaluate(batch_x, batch_y)\n",
    "        train_accuracy[i] = acc_train_accuracy/len(range(0, num_examples, BATCH_SIZE))\n",
    "        flip_accuracy[i] = evaluate(x_flip, y1hot_flip)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        #print(\"Train Accuracy = {:.3f}\".format(train_accuracy[i]))\n",
    "        print(\"Train Accuracy = {:.3f}; Flip Accuracy = {:.3f}\".format(train_accuracy[i],flip_accuracy[i]))\n",
    "        print()\n",
    "    saver.save(sess, './models/mnist-teacher-model')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    \"\"\"\n",
    "    We Then Do the Copy from Teacher to Student Models\n",
    "    \"\"\"\n",
    "    print('Before assignment')\n",
    "    #print(sess.run(weights_student['layer_1']))\n",
    "    sess.run(weights_student['layer_1'].assign(weights['layer_1']))\n",
    "    sess.run(weights_student['layer_2'].assign(weights['layer_2']))\n",
    "    sess.run(weights_student['fully_connected_1'].assign(weights['fully_connected_1']))\n",
    "    sess.run(weights_student['fully_connected_2'].assign(weights['fully_connected_2']))\n",
    "    sess.run(weights_student['out'].assign(weights['out']))\n",
    "    sess.run(biases_student['layer_1'].assign(biases['layer_1']))\n",
    "    sess.run(biases_student['layer_2'].assign(biases['layer_2']))\n",
    "    sess.run(biases_student['fully_connected_1'].assign(biases['fully_connected_1']))\n",
    "    sess.run(biases_student['fully_connected_2'].assign(biases['fully_connected_2']))\n",
    "    sess.run(biases_student['out'].assign(biases['out']))\n",
    "    print('After assignment')\n",
    "    #print(sess.run(weights_student['layer_1']))\n",
    "    \n",
    "    \"\"\"\n",
    "    Using Flipped Images to Unsupervised Training the Original Net\n",
    "    \"\"\"\n",
    "    train_accuracy_tf = np.zeros(EPOCHS_TF)\n",
    "    flip_accuracy_tf = np.zeros(EPOCHS_TF)\n",
    "    for i in range(EPOCHS_TF):\n",
    "        acc_loss = 0.0\n",
    "        X_train, X_train_flip = shuffle(x_rest, x_rest_flip)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x = X_train[offset:end]\n",
    "            batch_x_flip = X_train_flip[offset:end]\n",
    "            _, l = sess.run([training_tf_operation, loss_tf_op], feed_dict={x_feed: batch_x,\\\n",
    "                                                                            x_feed_student: batch_x_flip,\\\n",
    "                                                                            keep_prob: 1.0})\n",
    "            #print('Current loss in batch = {}'.format(l))\n",
    "            acc_loss += l\n",
    "        acc_loss /= num_examples\n",
    "        #train_accuracy_tf[i] = evaluate(x, y1hot)\n",
    "        train_accuracy_tf[i] = evaluate_student(x, y1hot)\n",
    "        flip_accuracy_tf[i] = evaluate_student(x_flip, y1hot_flip)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Acc loss = {}\".format(acc_loss))\n",
    "        print(\"Train Accuracy = {:.3f}; Flip Accuracy = {:.3f}\".format(train_accuracy_tf[i],flip_accuracy_tf[i]))\n",
    "        print()\n",
    "    saver.save(sess, './models/mnist-student-model')\n",
    "    print(\"Model saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
